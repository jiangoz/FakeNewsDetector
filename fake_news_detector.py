# -*- coding: utf-8 -*-
"""Fake News Detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xNSzBWZIx8vVOAAvCcGaNjfzPWCQ4GZm

<center> <h2> DS 3000 - Summer 2020</h2> </center>
<center> <h3> DS Report </h3> </center>

<center> <h3> Fake News Detector </h3> </center>
<center><h4> Yi Jiang, Kincent Lan, Richard Li </h4></center>

<hr style="height:2px; border:none; color:black; background-color:black;">

#### Executive Summary:

Amongst the political turmoil we are experiencing in the current age, we are fed many sources of misinformation that exacerbate the current problems of society. The Constitution of America declares that Americans have the right to the freedom of the press, and as such anyone can say what they want, regardless if it is correct or not. Although this a good thing, this does not negate the negative effects of misinformation spreading in society. Misinformation could lead to the public making uninformed decisions that may hurt society as a whole. We aim to help address this problem by attempting to distinguish what is fake news and what is not.

We obtained our dataset, which came from a CSV file, from the online website Kaggle. The data came in the form of title, text, date, and subject and contained no other information. As such, we data-wrangled statistics from the qualitative data to be able to detect and generalize characteristics of fake and real news. We also used the TfidVectorizer to obtain further additional information that may aid our classification problem. To visualize the dataset, we used the statistics we pulled out from the articles and made a scatterplot, heatmap, and a histogram. To analyze this dataset, we used the , k-Nearest Neighbor classifier, Multinomial NB: Naive Bayes, and Decision Tree classifier. When it came to hypertuning our algorithms, we used Gridsearch for cross-validation and testing.

<hr style="height:2px; border:none; color:black; background-color:black;">

## Outline
1. <a href='#1'>INTRODUCTION</a>
2. <a href='#2'>METHOD</a>
3. <a href='#3'>RESULTS</a>
4. <a href='#4'>DISCUSSION</a>

<a id="1"></a>
<hr style="height:2px; border:none; color:black; background-color:black;">

## 1. INTRODUCTION

**Problem Statement**

In this project, we will analyze the characteristics of news articles of fake and real news to determine how they can help distinguish between those two. In our time of turmoil and crisis in the political hemisphere, it is especially important to help the people classify between real and fake news.

This project will entail identifying potential variables, such as the date published, number of capital letters, number of exclamation marks, etc., to determine if they hae a signficant relationship in distinguishing between fake news and real news.

**Significance of the Problem**

Due to the political nature of our country, it is extremely important that everyone, regardless of their political views have access to correct information about major events. Nowadays, some news outlets are capable of spreading misinformation extremely quickly as people are growing more and more reliant on said outlets as the sole source information rather than factchecking things themselves. We are targeting articles by news journalists because that is how most people receive their information. Depending on the results of this study, this may be a solution to determine an easy way to determine if a news source might be fake or not.

**Synthesis**

Fake news detecting has been a topic of many data scientists have been trying to achieve. Our dataset, for example, came from someone who also tried to undertake a similar task, and their work of scraping the data greatly sped up our progress. 

Techniques used in text feature extraction are widely applied in other fields as well. The most popular usage, sentiment analysis, is used to determine whether a block of text is exhibiting a "positive" or "negative" connotation.

**Questions/Hypothesis**

1. Is it the case that the more capital letters there are in the title in a news source, the more likely that it is fake?
 * Null Hypothesis: There is no difference in the means of capital letters in the title between fake news and real news.
 * Alternative Hypothesis: There is a difference in the means of capital letters in the title between fake news and real news.


2. Is it the case that the more exclamation mark in the text of news source, the more likely that it is fake? 
 * Null Hypothesis: There is no difference in the means of exclamation marks between fake news and real news.
 * Alternative Hypothesis: There is a difference in the means of exclamation marks between fake news and real news.
 
 
3. Is it the case that the average sentence length for fake news would be less than real news?
 * Null Hypothesis: There is no difference in the means of average sentence length between fake news and real news.
 * Alternative Hypothesis: There is a difference in the means of average sentence length between fake news and real news.


4. Which machine learning model would be the best for predicting fake news from real news? Specifically these models: Multinomial NB: Naive Bayes, k-Nearest Neighbors, Decision Trees.

<a id="2"></a>
<hr style="height:2px; border:none; color:black; background-color:black;">

## 2. METHOD

### 2.1. Data Acquisition
"""

import pandas as pd
import numpy as np

# read the data
fake = pd.read_csv('https://raw.githubusercontent.com/KincentLan/news_dataset/master/Fake.csv')
real = pd.read_csv('https://raw.githubusercontent.com/KincentLan/news_dataset/master/True.csv')

fake.head()

real.head()

"""We obtained our data on fake news and real news from a Kaggle Dataset. The data includes the title of the article, the article itself, the subject of the article, and the date of the article. The dataset can be accessed here: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset?select=True.csv

The data contains all qualitative descriptions of past articles written from 2014-2019. Characteristics of these articles are as follows:

* Date - Standard date format, however, some of the dates were in the day-month-year format with the month abbreviated
* Title - String that contains the original title of the article
* Text - String that contains the original text of the article
* Subject - String that tells what type of article it is. 
  - Comes in a couple of catergories: 'News', 'politics', 'Government News', 'left-news', 'US_News', 'Middle-east', 'politicsNews', 'worldnews

### 2.2. Variables
#### Possible Independent Variables
* *date published*
* *title word count*
* *body word count*
* *title word count in all caps*
* *body word count in all caps*
* *Exclaimation mark count*
* *Average sentence length*
* *Features extracted using TfidfVectorizer*

#### Depedent Variable
* *classification of the news: fake or real*

#### Rationale
Since we are working with qualitative data, we need to first extract the features from the text. We came up with a list of potential features we can extract from the text through some data wrangling. We also would want to use the TfidfVectorizer to extract the significance of the word uses.

### 2.3. Data Analysis

We will be predicting wether a news article is real or fake using features extracted from textual data (e.g. word counts, word lengths, sentence lengths, amount of all caps words, amount of exclamation points, etc) as well as the date the news was published. We think date is important because in certain times such as the election season, there tend to be more misinformation being spread online. We think the textual features such as amount of all caps or amount of exclamation points can be an important factor because articles use them to grab attention or to make click baity titles to help them attract more viewers.

This is a supervised ML classification problem because we have a dataset for which we already know the outcome (we know if it’s fake or real news) and we want to learn to predict the outcome for new data. We want to learn a predictive model that maps features of the data (e.g. word count, date, news type, etc…) to an output (real or fake news). A fully trained machine learning model can then be used to make predictions to see if a news article is fake or real. This will be a classification learning task.

We decided to use three different classification algorithms to tackle this problem: kNN, Multinomial Naive Bayes, and Decision Tree Classifier. 

We used kNN because it is one the simplest algorithms and should take a relatively short amount of time to finish training. The algorithm predicts new data by looking at the k training samples that it's the closest to in distance. 

We chose Naive Bayes because it usually performs better when the data is of a large dimension, which in this case it is when extracted with the TfidfVectorizer. The algorithm takes in all the features and calculates the probability using Bayes rules and using it to predict the outcome of the input. 

Finally, we chose decision tree because we believe that an if-else chain can reasonably determine whether an article is real or fake. The decision tree is a hierarchy of if-else questions and lets us go from the observations to the target class.

<a id="3"></a>
<hr style="height:2px; border:none; color:black; background-color:black;">

## 3. RESULTS

### 3.1. Data Wrangling

#### Clean Prefix
The real news dataset has a prefix for most of its text field. We need to remove it first.
"""

def remove_reuter(text):
    if '(Reuters) - ' in text:
        return text.split('(Reuters) - ')[1]
    else: 
        return text
real['text'] = real['text'].apply(remove_reuter)
real.head()

"""#### Quantify DV"""

fake['target'] = 0
real['target'] = 1

"""#### Merge Fake with Real"""

data = pd.merge(fake, real, how='outer')

"""#### Quantify news_types with One-Hot Encoder"""

from sklearn.preprocessing import OneHotEncoder
new_df = data['subject'].values.reshape(-1, 1)

encoder = OneHotEncoder(sparse=False)
encoded_df = encoder.fit_transform(new_df)
features_df = pd.DataFrame(encoded_df, columns=encoder.get_feature_names())
# data = pd.merge(data, features_df, left_index=True, right_index=True)

data = data.drop(['subject'], axis=1)

"""#### Convert date to unix time"""

import time
import datetime

month_conv = {"January": 1, "February": 2, "March": 3, "April": 4, "May": 5, "June": 6, "July": 7, "August": 8, 
              "September": 9, "October": 10, "November": 11, "December": 12, 
              "Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6, "Jul": 7, "Aug": 8, 
              "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12}

def convert_date(s):
    try:
        s = s.replace(",", " ")
        date = s.split()
        if len(date) < 3:
            date = s.split("-")
            date[0] = int(date[0]) + 2000
            dtime = datetime.date(date[0], int(month_conv[date[1]]), int(date[2]))
            return time.mktime(dtime.timetuple())
        else:
            dtime = datetime.date(int(date[2]), int(month_conv[date[0]]), int(date[1]))
            return time.mktime(dtime.timetuple())
    except ValueError:
        pass

data["date"] = data["date"].map(convert_date)
data

"""#### Adds miscellaneous columns (word count, etc)"""

def get_word_count(string):
    return len(string.split(' '))
data['title_word_count'] = data['title'].apply(get_word_count)
data['text_word_count'] = data['text'].apply(get_word_count)

def get_upper_letter_count(string):
    return len([x for x in string.split() if x.isupper()])
data['text_all_cap_word_count'] = data['text'].apply(get_upper_letter_count)
data['title_all_cap_word_count'] = data['title'].apply(get_upper_letter_count)

def get_exclaim_count(string):
    ans = 0
    for i in string.split():
        if '!' in i:
            ans += 1
    return ans
data['title_exclaim_count'] = data['title'].apply(get_exclaim_count)
data['text_exclaim_count'] = data['text'].apply(get_exclaim_count)

def get_avg_sentence_length(string):
    x = string.replace('?', '.').replace('!', '.')
    sen = x.split('.')
    length = []
    for i in sen:
        length.append(len(i.split()))
    length = length[0:-1]
    try:
        return sum(length) / len(length)
    except ZeroDivisionError:
        pass

data['avg_sentence_len'] = data['text'].apply(get_avg_sentence_length)
data = data.dropna()
data

"""#### Feature extraction using TfidfVectorizer"""

features = data.iloc[:, 0:19].drop(['target'], axis=1)
target = data.target

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), min_df=5)
vectorizer.fit(X_train.text)
X_train_vectorized = vectorizer.transform(X_train.text)
X_test_vectorized = vectorizer.transform(X_test.text)

"""#### Feature selection using univariate"""

from sklearn.feature_selection import SelectKBest, f_regression

select = SelectKBest(score_func=f_regression, k=50)
select.fit(X_train_vectorized, y_train)

X_train_vectorized = select.transform(X_train_vectorized)
X_test_vectorized = select.transform(X_test_vectorized)

"""#### Drop text and title columns"""

X_train = X_train.drop(['title', 'text'], axis=1)
X_test = X_test.drop(['title', 'text'], axis=1)

"""#### Regularize data with MinMaxScaler"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(copy=True)
for column in X_train.columns:
    scaler.fit(X_train[column].values.reshape(-1, 1))
    X_train[column] =  scaler.transform(X_train[column].values.reshape(-1, 1))
    X_test[column] = scaler.transform(X_test[column].values.reshape(-1, 1))
X_train

X_test

## make sure the data is not negative from the scaling

def absolute_value_x(x):
    if not isinstance(x, str):
        return abs(x)
    return x

X_train = X_train.apply(absolute_value_x)
X_test = X_test.apply(absolute_value_x)

"""### 3.2. Data Exploration"""

import plotly.express as px
import datetime
def convert_target(t):
    if (t == 0):
        return "Fake News"
    else:
        return "Real News"
months = {"01":"January","02":"February","03":"March","04":"April","05":"May","06":"June","07":"July","08":"August","09":"September","10":"October","11":"November","12":"December"}

def convert_months(date):
    return months[datetime.datetime.fromtimestamp(date).strftime('%m')]

fig_data = data.copy()
fig_data["real_or_fake"] = fig_data["target"].apply(convert_target)
fig = px.scatter(fig_data, x="text_exclaim_count", y="text_all_cap_word_count", size="text_word_count", color="real_or_fake",range_x=[0,10],range_y=[0,200])
fig.update_layout(
    title_text='Article Word Count vs. Exclaimation Point Count vs. All-Cap Word Count', 
    xaxis_title_text='Number of exclaimation points in article', 
    yaxis_title_text='Number of all-cap words in article', 
)
# fig.show()

"""<img src="https://i.imgur.com/juDWsiw.png" width=700 height=450 />

This bubble chart plots the relationships between the number of exclaimation points in article (body) against the number of all-cap words in article (body).
There seems to be a small negative correlation in which the number of all-cap words decreases as number of exclaimation points increases in the article.
Also, the color allows us to see that fake news tend to have more exclaimation points in the article (body) as well as more all-cap words. 
Finally, the size of the bubble shows the word count of the entire news article. Fake news tend to have bigger bubbles than real news.
"""

fig2_data = data.copy()
fig2_data["month"] = fig2_data["date"].apply(convert_months)
fig2_data["real_or_fake"] = fig2_data["target"].apply(convert_target)
fig2 = px.histogram(fig2_data,x="month",color="real_or_fake")
fig2.update_layout(
    title_text='Amount of News Articles Published in Each Month', 
    xaxis_title_text='Months of the Year', 
    yaxis_title_text='Amount of News', 
)
# fig2.show()

"""<img src="https://i.imgur.com/KtjDVFs.png" width=700 height=450 />

This bar graph allows us the see the relationship between the months of a year and the amount of news published in that month.
The amount of fake news published stayed roughly the same throughout the year. Although, there seems to be a higher amount of real news being published 
in the last 4 months (September-December).
"""

fig3 = px.scatter_matrix(fig2_data, dimensions=["title_word_count","title_all_cap_word_count","title_exclaim_count"], color="real_or_fake",
title='Visualization of Textual Info in News Titles', height=700, labels={"title_word_count":"Word Count","title_all_cap_word_count":"All-Caps Word Count","title_exclaim_count":"Amount of \"!\""})
# fig3.show()

"""This scatterplot matrix lets us visualize the relationships between word count, all-caps word count, and amount of exclamation points in the news title.
Fake news tend to have more exclamation points, more words, and more all-caps words in the title.
There is a positive correlation between all-caps word count and word count in news title.
There is a small negative correlation between word count and the amount of exclamation points in the news title.

<img src="https://i.imgur.com/orEWWpv.png" width=700 height=700 />
"""

import seaborn as sns
fig4_data = data.copy()

sns.heatmap(fig4_data.corr(), square=True)

"""<img src="https://i.imgur.com/Gg1vz3f.png" width=700 height=600 />

This heatmap allows us to see the correlation between all the features in our dataset.
The "warmer" the color, the more positively correlated the two features are.
While the colder/darker colors show a negative correlation between the two features.

### 3.3. Model Construction

#### One-Way Anova Tests
"""

from scipy import stats

fake_real = ["Fake", "Real"]

def describe_data(data, IV, DV):
    return data.groupby(IV).agg(["count", "mean", "std", "sem"])[DV]

def oneway_ANOVA(data, IV, DV):
    df = describe_data(data, IV, DV)
    lists_interest = []
    lists_name = []
    for name in df.index:
        lists_interest.append(data[data[IV] == name][DV])
        lists_name.append(name)
        
    result = stats.f_oneway(*lists_interest)
    levene = stats.levene(*lists_interest)
    
    shapiro_results = []
    for i in range(0, len(lists_interest)):
        shapiro_results.append((lists_name[i], stats.shapiro(lists_interest[i])))
        
    print("-----------------------\nONE-WAY ANOVA RESULTS\n-----------------------")
    print("\n")
    
    print("F-test\n-------")
    print("F" + "(" + str(df["count"].size - 1) +","+ str(df["count"].sum() - df["count"].size) +") = " 
          + str(round(result[0], 2)) + ", p = " + str(round(result[1], 4)))
    print("\n")
    
    print("Assumption Checks\n-------------------")
    print()
    print("Assumption of Equality of Variances:")
    print("\tLeveneResult(statistic=" + str(levene[0]) + ", pvalue=" + str(levene[1]) + ")")
    if levene[1] < 0.05:
        print("\tAssumption is not met. p < 0.05")
    else:
        print("\tAssumption is met. p > 0.05")
    
    print()
    print("Assumption of Normality:")
    for n in shapiro_results:
        rf_bin, s_result = n
        print("\t" + fake_real[rf_bin] + " : (" + str(s_result[0]) + ", " + str(s_result[1]) + ")")
        if s_result[1] < 0.05:
            print("\tAssumption is not met. p < 0.05 \n")
        else:
            print("\tAssumption is met. p > 0.05 \n")

scaled_data = X_train.append(X_test)
scaled_data["target"] = data["target"]

"""#### Hypothesis 1
##### Is it the case that the more capital letters there are in the title in a news source, the more likely that it is fake?
"""

oneway_ANOVA(scaled_data, "target", "title_all_cap_word_count")

"""#### Test & Purpose
A one-way analysis of variance (ANOVA) was conducted to compare the mean amount of capital words in the title of fake news and real news articles.

#### Actual Results
Results revealed a statistically signficant difference between means of the capital words in the title between the fake news and real news.

Although the results showed a statistically signficant difference among means of the capital words in the title between the fake news and real news, this test is inconclusive because the equality of variance was not met. As such, this suggests that the way the two news groups were collected were not identical.

#### Hypothesis 2
Is it the case that the more exclamation mark in the text of news source, the more likely that it is fake?
"""

oneway_ANOVA(scaled_data, "target", "text_exclaim_count")

"""#### Test & Purpose

A one-way analysis of variance (ANOVA) was conducted to compare the mean amount of exclamation marks in the text of fake news and real news articles.

#### Actual Results
Results revealed a statistically signficant difference between means of the exclamation marks in the text between the fake news and real news.

Although the results showed a statistically signficant difference among means of the the exclamation marks in the text between the fake news and real news, this test is inconclusive because the equality of variance was not met. As such, this suggests that the way the two news groups were collected were not identical.

#### Hypothesis 3
##### Is it the case that the average sentence length for fake news would be less than real news?
"""

oneway_ANOVA(scaled_data, "target", "avg_sentence_len")

"""#### Test & Purpose
A one-way analysis of variance (ANOVA) was conducted to compare the means of the average sentence length of fake news and real news articles.

#### Actual Results
Results revealed a statistically signficant difference between means of the average sentence length between the fake news and real news.

Even though the assumptions of normality have not been met, the One-way ANOVA test is said to be robust enough to conclude a difference if there is one. Because the results showed a statistically signficant difference between means of the average sentence length between the fake news and real news, we accept the alternative hypothesis and say there is a difference between the means of the average sentence length of fake news and real news articles.

#### Machine Learning Algorithms

We decided to use three algorithms we learned in class on our 2 datasets. kNN, naive bayes, and decision tree.
For the vecotirzed dataset, we decided to let go of decision tree algorithm. The data contain too many dimensions for the algorithm to finish training in a timely manner.
"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report

kfold = KFold(n_splits=5, shuffle=True)

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier()
model.fit(X=X_train, y=y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
cv_mean = cross_val_score(estimator=model, X=X_train.append(X_test), y=y_train.append(y_test), cv=kfold).mean()
print(f'Cross-validation mean acc: {cv_mean:.2%}')
print('Classfication report for training:')
print(classification_report(y_true=y_train, y_pred=y_train_pred))
print('Classfication report for testing:')
print(classification_report(y_true=y_test, y_pred=y_test_pred))

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X=X_train, y=y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
cv_mean = cross_val_score(estimator=model, X=X_train.append(X_test), y=y_train.append(y_test), cv=kfold).mean()
print(f'Cross-validation mean acc: {cv_mean:.2%}')
print('Classfication report for training:')
print(classification_report(y_true=y_train, y_pred=y_train_pred))
print('Classfication report for testing:')
print(classification_report(y_true=y_test, y_pred=y_test_pred))

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X=X_train, y=y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
cv_mean = cross_val_score(estimator=model, X=X_train.append(X_test), y=y_train.append(y_test), cv=kfold).mean()
print(f'Cross-validation mean acc: {cv_mean:.2%}')
print('Classfication report for training:')
print(classification_report(y_true=y_train, y_pred=y_train_pred))
print('Classfication report for testing:')
print(classification_report(y_true=y_test, y_pred=y_test_pred))

from sklearn.neighbors import KNeighborsClassifier
from scipy.sparse import vstack

model = KNeighborsClassifier()
model.fit(X=X_train_vectorized, y=y_train)
y_train_pred = model.predict(X_train_vectorized)
y_test_pred = model.predict(X_test_vectorized)
cv_mean = cross_val_score(estimator=model, X=vstack([X_train_vectorized, X_test_vectorized]), y=y_train.append(y_test), cv=kfold).mean()
print(f'Cross-validation mean acc: {cv_mean:.2%}')
print('Classfication report for training:')
print(classification_report(y_true=y_train, y_pred=y_train_pred))
print('Classfication report for testing:')
print(classification_report(y_true=y_test, y_pred=y_test_pred))

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X=X_train_vectorized, y=y_train)
y_train_pred = model.predict(X_train_vectorized)
y_test_pred = model.predict(X_test_vectorized)
cv_mean = cross_val_score(estimator=model, X=vstack([X_train_vectorized, X_test_vectorized]), y=y_train.append(y_test), cv=kfold).mean()
print(f'Cross-validation mean acc: {cv_mean:.2%}')
print('Classfication report for training:')
print(classification_report(y_true=y_train, y_pred=y_train_pred))
print('Classfication report for testing:')
print(classification_report(y_true=y_test, y_pred=y_test_pred))

"""### 3.4. Model Evaluation

Based on the 5 models trained using both datasets and three different algorithms, the two best performing algorithms are the naive bayes (with vectorized data) as well as kNN (feature extraction). Both performed at an cross-validation accuracy at close to 94% % and an f1 score for both target values at 0.94.

Because of the nature of this classification task, it is extremely difficult, or impossible, to identify one single feature that determines the classification of a row in the dataset. As a result, the results won't help us confirm nor deny our hypotheses about how an individual feature is correlated to the article it represents being real or fake.

The discrepency between the scores of the training set and the testing set in our decision tree model indicates that it might be overfitting. It has an 100% accuracy when predicting new data when given the training set, but only a 93.57% accuracy when using the testing set. It performs relatively poor when given unseen data.

### 3.5. Model Optimization
We decided to use GridSearchCV to tune the hyperparameters on our naive bayes model (with vectorized training data), kNN model (with feature extracted data), and decision tree model. We chose the naive base model because it is our best performing model by far and we'd like to tune it even further. We chose the decision tree model because it is clearly overfitting and hypertunning it should give us an increase in performace. And we decided to include the kNN model as well for comparison purposes, even though it is unlikely for it to further improve its performance.

Note: we didn't combine the training & test set when doing GridSearchCV as our dataset is large enough already, and we'll eventually be testing the best model from the GridSearchCV on the testing set. It is important to make sure that the test and training data are separated.
"""

from sklearn.model_selection import GridSearchCV

param_grid = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 1, 10, 100]}
gridsearch_nb_vect = GridSearchCV(MultinomialNB(), param_grid, cv=5)
gridsearch_nb_vect.fit(X=X_train_vectorized, y=y_train)

gridsearch_nb_vect.best_params_

"""Naive base after tuning: alpha = 0.001"""

param_grid = {'max_depth': [5, 10, 15, 20]}
gridsearch_dt = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
gridsearch_dt.fit(X=X_train, y=y_train)

gridsearch_dt.best_params_

"""Decision Tree after tuning: max_depth = 10"""

param_grid = {'n_neighbors': [2, 3, 5, 7, 9]}
gridsearch_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
gridsearch_knn.fit(X=X_train, y=y_train)

gridsearch_knn.best_params_

"""kNN after tuning: n_neighbors = 7

### 3.6. Model Testing
"""

y_pred = gridsearch_nb_vect.predict(X_test_vectorized)
print('Classfication report for naive base:')

print(classification_report(y_true=y_test, y_pred=y_pred))

y_pred = gridsearch_dt.predict(X_test)

print('Classfication report for decision tree:')
print(classification_report(y_true=y_test, y_pred=y_pred))

y_pred = gridsearch_knn.predict(X_test)

print('Classfication report for kNN:')
print(classification_report(y_true=y_test, y_pred=y_pred))

"""After hypertuning, the accuracy for naive bayes and kNN did not experience much growth. 

The decision tree algorithm did experience an increase in its accuracy as expected, with anccuracy going up from 93.6% to 94%, and an increase in f1 score of 0.01. The model should no longer be overfitting.

## 4. DISCUSSION

### Summary

To summarize, we used a Kaggle dataset containing fake and real news sources containing the text, title, date published, and subject of the said news. In the data-wrangling process of this project, we removed the subject column to reduce the amount of bias and extracted numeric features manually, as well as using a TfidfVectorizer approach. We used unigrams and bigrams to better capture the context of the text. At the end at the data-wrangling process, we had two datasets, one with the features we manually extracted, and one from the TfidVectorizer. Also note that we used univariate selection to selection the 50 most important features from the TfidVectorizer dataset.

From the data we gathered manually, we scaled it using the MinMaxScaler and split it into training and testing sets.

Using both datasets, we fed the data to three machine learning algorithms: k-Nearest Neighbors, Multinomial Naive Bayes, and Decision Trees. For Decision Trees, we decided that the algorithm would perform too slow on the vectorized dataset so we did not consider it in this project. We retrieved the accuracy and F1 score to compare the algorithms to determine which one was the best. We hypertuned all three algorithms but only with the datasets that the perform the best with.

Using the dataset with the features we extracted manually, we used to perform One-Way ANOVA tests on the hypotheses listed in the introduction section. 

### Intepretation

We will now intepret the hypotheses/questions proposed in the introduction of this project:

##### 1. Is it the case that the more capital letters there are in the title in a news source, the more likely that it is fake?
The assumptions for our One-Way ANOVA test were not met, and although we retrieved a statistically signficant result, it is inconclusive that there is a signficant difference between the means of capital letters in the title of real news sources versus fake news sources.
   
   
##### 2. Is it the case that the more exclamation mark in the text of news source, the more likely that it is fake?
The assumptions for our One-Way ANOVA test were not met, also, and although we retrieved a statistically signficant result, it is inconclusive that there is a signficant difference between the means of exclamation marks in the text of real news sources versus fake news sources.


##### 3. Is it the case that the average sentence length for fake news would be less than real news?
Although the assumptions for normality for our One-Way ANOVA test were not met, the equality of variance was met. Because there was statistically signficant result for out test, and since the One-Way ANOVA is said to robust enough to ignore the assumption the normality, we accept the alternative hypothesis and conclude that the means of the average sentence length for fake news differs from real news.

##### 4. Which machine learning model would be the best for predicting fake news from real news? Specifically these models: Multinomial NB: Naive Bayes, k-Nearest Neighbors, Decision Trees.

We thought that Decision Trees was the best model because we initially believed that the data was aggregated, meaning the data biased, in such a way that if-else statements could represent said method perfectly. We also believe that the features of the articles were interdependent, so if-else statements would really suit the structure of the articles. However, after running the models, it seems that the accuracy levels between models are approximately the same. As such, we believe every model that we used is quite suitable.

### Conclusion

Looking back, we noticed that our dataset might be biased. As we visualized our data, we noticed that in certain timeframes, the news articles are predominantly one type over the other. Fundamentally, Naive Bayes algorithm assumes that all features are independent, but in this case many features are certainly interdependent. For example, the word "united" and the word "states" often appears in pairs, and so they cannot be said to be independent. Furthermore, we used bigrams to capture the contextual meaning of the text. By grouping them into pairs, we introducing them into interdependency of the text. We chose Naive Bayes because it worked well with high dimensional data, but looking back we could have chosen another algorithm that could have suited need better.


We chose kNN because it is a simple algorithm that works best with relatively lower dimension data. This unfortunately means that we could not run the algorithm on the raw data we processed directly from the TfidfVectorizer as it has too many columns/features. Instead, we used univariate feature selection to reduce the number of features down to 50. We lost a significant amount of data in that process, but even so, 50-dimension data still would end up creating a very complex model using the kNN algorithm, which is often prone to overfitting. 


As for the Decision Tree, we still believe that it is a good model to represent the structure of the data, however it is very slow in processing data in general. As such, we believe that there exist a better algorithm that may better represent the structure of the data.

<a id="5"></a>
<hr style="height:2px; border:none; color:black; background-color:black;">

### CONTRIBUTIONS
* Describe each team member's contributions to the report (who did what in each section)
* Remember this is a team effort!
* Each member of your team will provide peer evaluation of other team members. Your final grade on the project will be based on those peer evaluations. An survey will be shared after the deadline for this deliverable.

Kincent did the introduction, One-Way ANOVA tests, hypertuning, summaries and most of the conclusion. 


Richard did data-wrangling, data-acquisition, model evaluation, model optimization, model testing.



Yi did data-exploration, some of the summaries, and data analysis.
"""

